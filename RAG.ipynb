{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    "    Settings,\n",
    ")\n",
    "from llama_parse import LlamaParse\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "\n",
    "# NESTED ASYNCIO LOOP NEEDED TO RUN ASYNC IN A NOTEBOOK\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# check if storage already exists\n",
    "PERSIST_DIR = \"./storage\"\n",
    "\n",
    "# bge-large embedding model\n",
    "# Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-zh-v1.5\")\n",
    "\n",
    "\n",
    "# create the parser\n",
    "parser = LlamaParse(result_type=\"markdown\")\n",
    "\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "# chunk\n",
    "text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "if not os.path.exists(PERSIST_DIR):\n",
    "    # load the documents\n",
    "    documents = SimpleDirectoryReader(\"data\",file_extractor=file_extractor).load_data()\n",
    "    # 將 documents 存成二進位檔案\n",
    "    with open(\"documents.pkl\", \"wb\") as f:\n",
    "        pickle.dump(documents, f)\n",
    "    # create the index\n",
    "    index = VectorStoreIndex.from_documents(documents, transformations=[text_splitter])\n",
    "    \n",
    "    # store it for later\n",
    "    index.storage_context.persist(persist_dir=PERSIST_DIR)\n",
    "else:\n",
    "    # load the existing index\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)\n",
    "    index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "Settings.llm = Ollama(model=\"llama3.1:latest\", request_timeout=60.0,temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"校園行事曆有甚麼活動？請詳細列出\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each will retrieve the top-2 most similar nodes\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# configure retriever\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2, verbose=True)\n",
    "# vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=3, verbose=True)\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=3,\n",
    "    num_queries=4,  # set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    "    # query_gen_prompt=\"...\",  # we could override the query generation prompt here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer()\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.7)],\n",
    "    \n",
    ")\n",
    "response = query_engine.query(\"一月有甚麼活動？\")\n",
    "print(response)\n",
    "\n",
    "# file_names = [os.path.basename(path) for path in response.references]\n",
    "# print(\n",
    "#     f\"response: \\n answer_content: {response.answer}\\n Reference_file_path: {file_names}\"\n",
    "# )\n",
    "print(\"-----------------\")\n",
    "for i, node in enumerate(response.source_nodes):\n",
    "    print(f\"Node {i + 1}:\")\n",
    "    for key, value in node.metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(f\"content: {node.get_content()}\")\n",
    "    print(\"-----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_dict = query_engine.get_prompts()\n",
    "for k, p in prompts_dict.items():\n",
    "    print(f\"Prompt Key: {k}\\nText:\\n{p.get_template()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply nested async to run in a notebook\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "nodes_with_scores = retriever.retrieve(query)\n",
    "\n",
    "for node in nodes_with_scores:\n",
    "    print(f\"Score: {node.score:.2f} - {node.text}...\\n-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt\n",
    "qa_prompt_tmpl_str = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Given the context information and not prior knowledge, \"\n",
    "    \"answer the query in the style of a Shakespeare play.\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "# prompt_tmpl = PromptTemplate(\n",
    "#     qa_prompt_tmpl_str,\n",
    "#     context_str=\"\\n\\n\".join([n.get_content() for n in retrieved_nodes]),\n",
    "#     query_str=query,\n",
    "# )\n",
    "# # configure response synthesizer\n",
    "# response_synthesizer = get_response_synthesizer(text_qa_template=qa_prompt_tmpl_str)\n",
    "\n",
    "\n",
    "query = \"What is the capital of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 過濾固定無用問句\n",
    "def clean_examples(examples, num_questions_per_chunk):\n",
    "    cleaned_examples = []\n",
    "\n",
    "    for idx, example in enumerate(examples):\n",
    "        # 根據num_questions_per_chunk的值刪除索引對應的資料\n",
    "        if idx % num_questions_per_chunk != 0:  # 保留1, 3, 5, 7...\n",
    "            cleaned_examples.append(example)\n",
    "\n",
    "    return cleaned_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from types import SimpleNamespace\n",
    "# check if dataset already exists\n",
    "PERSIST_DATASET_FILE = \"./rag_dataset.json\"\n",
    "\n",
    "if not os.path.exists(PERSIST_DATASET_FILE):\n",
    "    # 讀取 documents\n",
    "    with open(\"documents.pkl\", \"rb\") as f:\n",
    "        documents = pickle.load(f)\n",
    "\n",
    "    QUESTION_GEN_QUERY = \"你是一名教師/教授。你的任務是為即將到來的測驗/考試設計 {num_questions_per_chunk} 道題目。這些題目應該涵蓋文件中的多樣內容，並且限定在所提供的上下文資訊內。\"\n",
    "    QUESTION_GENERATION_PROMPT = \"\"\"\\\n",
    "    以下是背景資訊：\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "    根據上述背景資訊，而非先前知識(prior knowledge)，  \n",
    "    僅基於以下查詢生成問題：  \n",
    "    {query_str}\n",
    "    \"\"\"\n",
    "    NUM_QUESTIONS_PER_CHUNK = 2\n",
    "\n",
    "    # define generator, generate questions\n",
    "    dataset_generator = RagDatasetGenerator.from_documents(\n",
    "        documents=documents,\n",
    "        num_questions_per_chunk= NUM_QUESTIONS_PER_CHUNK,  # set the number of questions per nodes\n",
    "        transformations=[text_splitter],\n",
    "        question_gen_query=QUESTION_GEN_QUERY,\n",
    "    )\n",
    "\n",
    "    # rag_dataset = dataset_generator.generate_questions_from_nodes()\n",
    "    rag_dataset = await dataset_generator.agenerate_dataset_from_nodes()\n",
    "\n",
    "    # clean the examples\n",
    "    rag_dataset.examples = clean_examples(rag_dataset.examples, NUM_QUESTIONS_PER_CHUNK)\n",
    "\n",
    "    # save this dataset as it is required for the submission\n",
    "    rag_dataset.save_json(\"rag_dataset.json\")\n",
    "else:\n",
    "    import json\n",
    "    with open(PERSIST_DATASET_FILE, \"r\") as f:\n",
    "        # 將字典轉換為支持屬性訪問的結構\n",
    "        rag_dataset = json.load(f, object_hook=lambda d: SimpleNamespace(**d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.evaluation import (\n",
    "    EvaluationResult,\n",
    "    RelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    ")\n",
    "from llama_index.core import Response\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from llama_index.core.llama_dataset import (\n",
    "    LabelledRagDataExample,\n",
    ")\n",
    "# 累積評估結果的 DataFrame\n",
    "eval_results_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# 累積顯示評估結果的函數\n",
    "def display_eval_df(\n",
    "    response: Response,\n",
    "    dataset_example: LabelledRagDataExample,\n",
    "    relevancy_eval_result: EvaluationResult,\n",
    "    correctness_eval_result: EvaluationResult,\n",
    ") -> None:\n",
    "    if not response.source_nodes:\n",
    "        print(\"no response!\")\n",
    "        return\n",
    "    eval_row = pd.DataFrame(\n",
    "        [\n",
    "            {   \"Query\": dataset_example.query,\n",
    "                \"Response\": str(response),\n",
    "                \"Reference Answer\": dataset_example.reference_answer,\n",
    "                \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n",
    "                \"Relevancy Eval Result\": f\"{'Pass' if relevancy_eval_result.passing else 'Fail'} \\nscore: {relevancy_eval_result.score}\",\n",
    "                \"Relevancy Reasoning\": relevancy_eval_result.feedback,\n",
    "                \"Correctness Eval Result\": f\"{'Pass' if correctness_eval_result.passing else 'Fail'} \\n\\nscore: {correctness_eval_result.score}\",\n",
    "                \"Correctness Reasoning\": correctness_eval_result.feedback,\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    global eval_results_df\n",
    "    eval_results_df = pd.concat([eval_results_df, eval_row], ignore_index=True)\n",
    "\n",
    "\n",
    "# 評估查詢引擎並顯示最終結果\n",
    "async def evaluate_chat_engine(chat_engine, dataset_examples:LabelledRagDataExample):\n",
    "    dataset_examples = dataset_examples\n",
    "    questions = [e.query for e in dataset_examples]\n",
    "\n",
    "    c = [chat_engine.aquery(q) for q in questions]\n",
    "    results = await asyncio.gather(*c)\n",
    "    print(\"finished query\")\n",
    "    print('------------------------------')\n",
    "\n",
    "    # 定義評估器\n",
    "    relevancy_evaluator = RelevancyEvaluator()\n",
    "    correctness_evaluator = CorrectnessEvaluator()\n",
    "\n",
    "    relevancy_total_correct = 0\n",
    "    correctness_total_correct = 0\n",
    "    for response, dataset_example in zip(results, dataset_examples):\n",
    "        relevancy_eval_result = relevancy_evaluator.evaluate_response(\n",
    "            query=dataset_example.query, response=response\n",
    "        )\n",
    "        correctness_eval_result = correctness_evaluator.evaluate_response(\n",
    "            query=dataset_example.query,\n",
    "            response=response,\n",
    "            reference=dataset_example.reference_answer,\n",
    "        )\n",
    "\n",
    "        # 累積每個查詢的評估結果\n",
    "        display_eval_df(\n",
    "            response, dataset_example, relevancy_eval_result, correctness_eval_result\n",
    "        )\n",
    "\n",
    "        if relevancy_eval_result.passing:\n",
    "            relevancy_total_correct += 1\n",
    "        if correctness_eval_result.passing:\n",
    "            correctness_total_correct += 1\n",
    "\n",
    "    return relevancy_total_correct, correctness_total_correct, len(results)\n",
    "\n",
    "\n",
    "# 執行評估查詢並顯示最終累積結果\n",
    "def run_evaluate_chat_engine(chat_engine, dataset_examples):\n",
    "    relevancy_total_correct, correctness_total_correct, total_questions = asyncio.run(\n",
    "        evaluate_chat_engine(chat_engine, dataset_examples)\n",
    "    )\n",
    "    global eval_results_df\n",
    "    # styled_df = eval_results_df.style.set_properties(\n",
    "    #     **{\"inline-size\": \"400px\", \"overflow-wrap\": \"break-word\"},\n",
    "    #     subset=[\"Response\", \"Source\"]\n",
    "    # )\n",
    "    styled_df = eval_results_df.style.set_properties(\n",
    "        **{\"white-space\": \"pre-wrap\"}, \n",
    "    )\n",
    "\n",
    "    display(styled_df)\n",
    "    # 顯示總結結果\n",
    "    print(f\"Total Relevancy correct: {relevancy_total_correct} out of {total_questions}\")\n",
    "    print(\n",
    "        f\"Total Correctness correct: {correctness_total_correct} out of {total_questions}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "Here are three search queries related to your input:\n",
      "1. 未來大學校園活動安排\n",
      "2. 大學行事曆和重要日期\n",
      "3. 學校生活和未來大學校園文化介紹\n",
      "Generated queries:\n",
      "Here are three search queries related to the input:\n",
      "1. 九月份學校開學典禮\n",
      "2. 學校新學期入學安排\n",
      "3. 九月份教育局活動清單\n",
      "finished query\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_f7f24_row0_col0, #T_f7f24_row0_col1, #T_f7f24_row0_col2, #T_f7f24_row0_col3, #T_f7f24_row0_col4, #T_f7f24_row0_col5, #T_f7f24_row0_col6, #T_f7f24_row0_col7, #T_f7f24_row1_col0, #T_f7f24_row1_col1, #T_f7f24_row1_col2, #T_f7f24_row1_col3, #T_f7f24_row1_col4, #T_f7f24_row1_col5, #T_f7f24_row1_col6, #T_f7f24_row1_col7 {\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_f7f24\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_f7f24_level0_col0\" class=\"col_heading level0 col0\" >Query</th>\n",
       "      <th id=\"T_f7f24_level0_col1\" class=\"col_heading level0 col1\" >Response</th>\n",
       "      <th id=\"T_f7f24_level0_col2\" class=\"col_heading level0 col2\" >Reference Answer</th>\n",
       "      <th id=\"T_f7f24_level0_col3\" class=\"col_heading level0 col3\" >Source</th>\n",
       "      <th id=\"T_f7f24_level0_col4\" class=\"col_heading level0 col4\" >Relevancy Eval Result</th>\n",
       "      <th id=\"T_f7f24_level0_col5\" class=\"col_heading level0 col5\" >Relevancy Reasoning</th>\n",
       "      <th id=\"T_f7f24_level0_col6\" class=\"col_heading level0 col6\" >Correctness Eval Result</th>\n",
       "      <th id=\"T_f7f24_level0_col7\" class=\"col_heading level0 col7\" >Correctness Reasoning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_f7f24_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_f7f24_row0_col0\" class=\"data row0 col0\" >什麼是未來大學校園行事曆?</td>\n",
       "      <td id=\"T_f7f24_row0_col1\" class=\"data row0 col1\" >未來大學校園行事曆是一份綜合了各項校園活動的日曆，涵蓋了學生的權益、環保義工、創業中心開放日等多個方面。</td>\n",
       "      <td id=\"T_f7f24_row0_col2\" class=\"data row0 col2\" >未來大學校園行事曆是一份包含各月活動安排的清單，涵蓋健康日、文化節、環保義工日、創業中心開放日等多種活動。</td>\n",
       "      <td id=\"T_f7f24_row0_col3\" class=\"data row0 col3\" ># 九月\n",
       "\n",
       "# 9月5日：新學年開學典禮\n",
       "\n",
       "地點：大禮堂\n",
       "\n",
       "內容：校長致辭、新生介紹、校園生活指導\n",
       "\n",
       "# 9月20日：學生權益講座\n",
       "\n",
       "地點：學生權益保護機構\n",
       "\n",
       "內容：學生權益保護講座、法律援助介紹\n",
       "\n",
       "# 十月\n",
       "\n",
       "# 10月10日：校園節能日\n",
       "\n",
       "地點：校園各處\n",
       "\n",
       "內容：節能設備展示、節能比賽\n",
       "\n",
       "# 10月24日：國際交流活動\n",
       "\n",
       "地點：國際合作中心\n",
       "\n",
       "內容：留學生交流、國際文化展示\n",
       "\n",
       "# 十一月\n",
       "\n",
       "# 11月8日：科技創新大會\n",
       "\n",
       "地點：科研與創新區\n",
       "\n",
       "內容：科技創新展示、專題研討會\n",
       "\n",
       "# 十二月\n",
       "\n",
       "# 12月21日：聖誕慶祝活動\n",
       "\n",
       "地點：校園廣場\n",
       "\n",
       "內容：聖誕樹點燈、音樂表演、慈善義賣...</td>\n",
       "      <td id=\"T_f7f24_row0_col4\" class=\"data row0 col4\" >Pass \n",
       "score: 1.0</td>\n",
       "      <td id=\"T_f7f24_row0_col5\" class=\"data row0 col5\" >YES</td>\n",
       "      <td id=\"T_f7f24_row0_col6\" class=\"data row0 col6\" >Pass \n",
       "\n",
       "score: 4.0</td>\n",
       "      <td id=\"T_f7f24_row0_col7\" class=\"data row0 col7\" >The generated answer is highly relevant to the user query, and most of the information is correct. The only minor difference is that the reference answer mentions \"健康日\" (health day) which is not explicitly mentioned in the generated answer, but it's implied by \"學生的權益\" (students' rights). Overall, the generated answer provides a comprehensive overview of the university calendar, and its correctness score is 4.0.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_f7f24_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_f7f24_row1_col0\" class=\"data row1 col0\" >什麼是九月份學校的首要活動？</td>\n",
       "      <td id=\"T_f7f24_row1_col1\" class=\"data row1 col1\" >新學年開學典禮。</td>\n",
       "      <td id=\"T_f7f24_row1_col2\" class=\"data row1 col2\" >新學年開學典禮。</td>\n",
       "      <td id=\"T_f7f24_row1_col3\" class=\"data row1 col3\" ># 九月\n",
       "\n",
       "# 9月5日：新學年開學典禮\n",
       "\n",
       "地點：大禮堂\n",
       "\n",
       "內容：校長致辭、新生介紹、校園生活指導\n",
       "\n",
       "# 9月20日：學生權益講座\n",
       "\n",
       "地點：學生權益保護機構\n",
       "\n",
       "內容：學生權益保護講座、法律援助介紹\n",
       "\n",
       "# 十月\n",
       "\n",
       "# 10月10日：校園節能日\n",
       "\n",
       "地點：校園各處\n",
       "\n",
       "內容：節能設備展示、節能比賽\n",
       "\n",
       "# 10月24日：國際交流活動\n",
       "\n",
       "地點：國際合作中心\n",
       "\n",
       "內容：留學生交流、國際文化展示\n",
       "\n",
       "# 十一月\n",
       "\n",
       "# 11月8日：科技創新大會\n",
       "\n",
       "地點：科研與創新區\n",
       "\n",
       "內容：科技創新展示、專題研討會\n",
       "\n",
       "# 十二月\n",
       "\n",
       "# 12月21日：聖誕慶祝活動\n",
       "\n",
       "地點：校園廣場\n",
       "\n",
       "內容：聖誕樹點燈、音樂表演、慈善義賣...</td>\n",
       "      <td id=\"T_f7f24_row1_col4\" class=\"data row1 col4\" >Pass \n",
       "score: 1.0</td>\n",
       "      <td id=\"T_f7f24_row1_col5\" class=\"data row1 col5\" >YES. The response \"新學年開學典禮\" is in line with the context information provided, which lists a series of events happening throughout the year, and September 5th being the day for the new school year opening ceremony.</td>\n",
       "      <td id=\"T_f7f24_row1_col6\" class=\"data row1 col6\" >Pass \n",
       "\n",
       "score: 5.0</td>\n",
       "      <td id=\"T_f7f24_row1_col7\" class=\"data row1 col7\" >The generated answer is identical to the reference answer, indicating that it is fully correct and relevant to the user query. The fact that it provides the same information as the reference answer suggests a high level of accuracy and relevance.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x15bb6e59c40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Relevancy correct: 2 out of 2\n",
      "Total Correctness correct: 2 out of 2\n"
     ]
    }
   ],
   "source": [
    "# 執行查詢引擎並評估結果\n",
    "run_evaluate_chat_engine(query_engine, rag_dataset.examples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
